{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "343d83c4",
   "metadata": {},
   "source": [
    "### Deep Learning for Computer Vision  \n",
    "### Multi-Task Regression with the Digital Typhoon Dataset\n",
    "\n",
    "This notebook demonstrates a **supervised multi-task regression** workflow for remote sensing using **TorchGeo** using the Digital Typhoon dataset, which consists of infrared (IR) satellite imagery of tropical cyclones paired with meteorological measurements.\n",
    "\n",
    "The objective is to predict multiple continuous typhoon intensity variables from satellite imagery using a deep learning model.  \n",
    "\n",
    "#### Dataset Overview\n",
    "The [Digital Typhoon](https://torchgeo.readthedocs.io/en/stable/api/datasets.html#digital-typhoon) is derived from hourly infrared channel observations captured by multiple generations of the Himawari meteorological satellites, spanning the period from 1978 to the present. The satellite measurements have been converted to brightness temperatures and normalized across different sensors, resulting in a consistent spatio-temporal dataset covering more than four decades.  \n",
    "\n",
    "**Dataset features:**\n",
    "- Infrared (IR) satellite imagery of 512 × 512 pixels at ~5km resolution \n",
    "- Auxiliary metadata including wind speed, pressure and additional typhoon-related attributes  \n",
    "- 1,099 typhoons and 189,364 images\n",
    "\n",
    "**References**  \n",
    "Digital Typhoon Dataset: *A Large-Scale Benchmark for Tropical Cyclone Analysis*      [arXiv:2411.16421](https://arxiv.org/pdf/2411.16421) ; [arXiv:2311.02665](https://arxiv.org/pdf/2311.02665)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cfcd2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchgeo.datasets import DigitalTyphoon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6f21ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "root = \"/home/ogallo/DL4CV/DigitalTyphoon\"\n",
    "\n",
    "dataset = DigitalTyphoon(\n",
    "    root=root,\n",
    "    features=[\"wind\", \"pressure\"],\n",
    "    targets=[\"wind\", \"pressure\"],\n",
    "    sequence_length=1,\n",
    "    download=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc18a526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173418\n",
      "{'image': tensor([[[0.7248, 0.7813, 0.7813,  ..., 0.9331, 0.9363, 0.9331],\n",
      "         [0.7248, 0.7576, 0.7735,  ..., 0.9331, 0.9331, 0.9331],\n",
      "         [0.7290, 0.7536, 0.7656,  ..., 0.9299, 0.9299, 0.9331],\n",
      "         ...,\n",
      "         [0.6904, 0.6495, 0.6007,  ..., 0.8483, 0.8798, 0.8659],\n",
      "         [0.6542, 0.6400, 0.6725,  ..., 0.8483, 0.8659, 0.8447],\n",
      "         [0.7373, 0.7536, 0.7967,  ..., 0.8518, 0.8518, 0.8483]]]), 'wind': tensor(-1.1229), 'pressure': tensor(0.5422), 'label': tensor([-1.1229,  0.5422])}\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))        # number of sequences\n",
    "print(dataset[0])          # inspect the first sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e74c63c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id                   image_path  year  month  day  hour  grade    lat  \\\n",
      "0  197830  1978120100-197830-GMS1-1.h5  1978     12    1     0      6  36.00   \n",
      "1  197830  1978120103-197830-GMS1-1.h5  1978     12    1     3      6  37.46   \n",
      "2  197830  1978120106-197830-GMS1-1.h5  1978     12    1     6      6  39.00   \n",
      "3  197901  1978123112-197901-GMS1-1.h5  1978     12   31    12      2   2.00   \n",
      "4  197901  1978123116-197901-GMS1-1.h5  1978     12   31    16      2   2.30   \n",
      "\n",
      "      lng  pressure  wind  dir50  long50  short50  dir30  long30  short30  \\\n",
      "0  174.00     996.0   0.0      0       0        0      0       0        0   \n",
      "1  176.44     994.0   0.0      0       0        0      0       0        0   \n",
      "2  179.00     992.0   0.0      0       0        0      0       0        0   \n",
      "3  172.00    1004.0   0.0      0       0        0      0       0        0   \n",
      "4  171.81    1002.7   0.0      0       0        0      0       0        0   \n",
      "\n",
      "   landfall  intp  \n",
      "0         0     0  \n",
      "1         0     1  \n",
      "2         0     0  \n",
      "3         0     0  \n",
      "4         0     1  \n"
     ]
    }
   ],
   "source": [
    "aux_data = pd.read_csv(\"/home/ogallo/DL4CV/DigitalTyphoon/WP/aux_data.csv\")\n",
    "print(aux_data.head())     # inspect auxiliary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0216a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1978, 1979, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989,\n",
       "       1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000,\n",
       "       2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011,\n",
       "       2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_data['year'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d76f04",
   "metadata": {},
   "source": [
    "Subset the dataset based on the typhoon grade, number of typhoons and lifecycle??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "674522d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original dataset:\n",
      "  Total images: 189,364\n",
      "  Total typhoons: 1099\n",
      "  Year range: 1978 - 2022\n",
      "  Years available: 44\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "root = \"/home/ogallo/DL4CV/DigitalTyphoon/WP\"\n",
    "output_dir = \"/home/ogallo/DL4CV/WP_sampled_50\"\n",
    "total_typhoons = 50\n",
    "images_per_typhoon = 50  # ← Target images per typhoon \n",
    "np.random.seed(42)\n",
    "\n",
    "# load auxiliary data\n",
    "aux_path = os.path.join(root, \"aux_data.csv\")\n",
    "df = pd.read_csv(aux_path)\n",
    "\n",
    "print(f\"\\nOriginal dataset:\")\n",
    "print(f\"  Total images: {len(df):,}\")\n",
    "print(f\"  Total typhoons: {df['id'].nunique()}\")\n",
    "print(f\"  Year range: {df['year'].min()} - {df['year'].max()}\")\n",
    "print(f\"  Years available: {df['year'].nunique()}\")\n",
    "\n",
    "# typhoon summary statistics\n",
    "typhoon_info = df.groupby('id').agg({\n",
    "    'year': 'min',\n",
    "    'grade': 'max'\n",
    "}).reset_index().rename(columns={'year':'first_year', 'grade':'peak_grade'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "237b8e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available years: 44 (1978 - 2022)\n",
      "Target years for sampling: 44\n",
      "Sampling every ~1 year(s)\n",
      "\n",
      "Selected 44 typhoons\n",
      "Year range in sample: 1978 - 2022\n",
      "Unique years covered: 44\n",
      "\n",
      "Grade distribution in sampled typhoons:\n",
      "  Grade 3: 11 typhoons\n",
      "  Grade 4: 7 typhoons\n",
      "  Grade 5: 9 typhoons\n",
      "  Grade 6: 16 typhoons\n",
      "  Grade 7: 1 typhoons\n"
     ]
    }
   ],
   "source": [
    "# stratified sampling of typhoons by year\n",
    "all_years = sorted(typhoon_info['first_year'].unique())\n",
    "print(f\"\\nAvailable years: {len(all_years)} ({all_years[0]} - {all_years[-1]})\")\n",
    "\n",
    "# Create target years - evenly distributed\n",
    "num_years = len(all_years)\n",
    "step = max(1, num_years // total_typhoons)\n",
    "target_year_indices = list(range(0, num_years, step))[:total_typhoons]\n",
    "target_years = [all_years[i] for i in target_year_indices]\n",
    "\n",
    "print(f\"Target years for sampling: {len(target_years)}\")\n",
    "print(f\"Sampling every ~{step} year(s)\")\n",
    "\n",
    "sampled_typhoons = []\n",
    "\n",
    "for year in target_years:\n",
    "    candidates = typhoon_info[typhoon_info['first_year'] == year]\n",
    "    if len(candidates) > 0:\n",
    "        selected = candidates.iloc[0]\n",
    "        sampled_typhoons.append(int(selected['id']))\n",
    "\n",
    "print(f\"\\nSelected {len(sampled_typhoons)} typhoons\")\n",
    "\n",
    "# Verify year distribution\n",
    "sampled_info = typhoon_info[typhoon_info['id'].isin(sampled_typhoons)]\n",
    "print(f\"Year range in sample: {sampled_info['first_year'].min()} - {sampled_info['first_year'].max()}\")\n",
    "print(f\"Unique years covered: {sampled_info['first_year'].nunique()}\")\n",
    "\n",
    "# Grade distribution\n",
    "print(f\"\\nGrade distribution in sampled typhoons:\")\n",
    "grade_dist = sampled_info['peak_grade'].value_counts().sort_index()\n",
    "for grade, count in grade_dist.items():\n",
    "    print(f\"  Grade {grade}: {count} typhoons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71bcfc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 2,080\n",
      "Unique typhoons: 44\n",
      "Unique years: 44\n",
      "Average images per typhoon: 47.3\n"
     ]
    }
   ],
   "source": [
    "# lifecycle-based image sampling per typhoon\n",
    "all_sampled_rows = []\n",
    "\n",
    "for typhoon_id in sampled_typhoons:\n",
    "    # Get ALL rows for this typhoon\n",
    "    typhoon_data = df[df['id'] == typhoon_id].copy()\n",
    "    typhoon_data = typhoon_data.sort_values(['year','month','day','hour']).reset_index(drop=True)\n",
    "    n_images = len(typhoon_data)\n",
    "    \n",
    "    # Decide how many images to sample\n",
    "    if n_images <= images_per_typhoon:\n",
    "        # Take ALL images if typhoon has fewer than target\n",
    "        sampled_rows = typhoon_data\n",
    "    else:\n",
    "        # Sample from lifecycle\n",
    "        n_sample = images_per_typhoon\n",
    "        \n",
    "        # Always include: early, peak, decay\n",
    "        peak_idx = typhoon_data['grade'].idxmax()\n",
    "        early_idx = 0\n",
    "        decay_idx = n_images - 1\n",
    "        \n",
    "        sampled_indices = {early_idx, peak_idx, decay_idx}\n",
    "        \n",
    "        # Fill remaining with evenly spaced samples across lifecycle\n",
    "        remaining = n_sample - len(sampled_indices)\n",
    "        if remaining > 0:\n",
    "            available = list(set(range(n_images)) - sampled_indices)\n",
    "            if len(available) > 0:\n",
    "                # Strategy 1: Evenly spaced (better lifecycle coverage)\n",
    "                step_size = max(1, len(available) // remaining)\n",
    "                evenly_spaced = available[::step_size][:remaining]\n",
    "                sampled_indices.update(evenly_spaced)\n",
    "                \n",
    "                # If we still need more, fill with random\n",
    "                if len(sampled_indices) < n_sample:\n",
    "                    still_available = list(set(available) - set(evenly_spaced))\n",
    "                    if still_available:\n",
    "                        additional_needed = n_sample - len(sampled_indices)\n",
    "                        additional = np.random.choice(\n",
    "                            still_available, \n",
    "                            size=min(additional_needed, len(still_available)), \n",
    "                            replace=False\n",
    "                        )\n",
    "                        sampled_indices.update(additional)\n",
    "        \n",
    "        sampled_rows = typhoon_data.iloc[list(sampled_indices)]\n",
    "    \n",
    "    all_sampled_rows.append(sampled_rows)\n",
    "\n",
    "# Concatenate all sampled rows\n",
    "df_sampled = pd.concat(all_sampled_rows, ignore_index=True)\n",
    "\n",
    "print(f\"Total images: {len(df_sampled):,}\")\n",
    "print(f\"Unique typhoons: {df_sampled['id'].nunique()}\")\n",
    "print(f\"Unique years: {df_sampled['year'].nunique()}\")\n",
    "print(f\"Average images per typhoon: {len(df_sampled) / df_sampled['id'].nunique():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "765d2cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying 2080 images...\n",
      "\n",
      "Copied 2080/2080 files\n"
     ]
    }
   ],
   "source": [
    "# copy sampled images to output directory\n",
    "images_src_dir = os.path.join(root, \"image\")\n",
    "images_dst_dir = os.path.join(output_dir, \"image\")\n",
    "os.makedirs(images_dst_dir, exist_ok=True)\n",
    "\n",
    "copied = 0\n",
    "not_found = []\n",
    "\n",
    "print(f\"Copying {len(df_sampled)} images...\")\n",
    "for idx, row in df_sampled.iterrows():\n",
    "    img_file = row['image_path']\n",
    "    found = False\n",
    "    \n",
    "    for root_dir, dirs, files in os.walk(images_src_dir):\n",
    "        if img_file in files:\n",
    "            src = os.path.join(root_dir, img_file)\n",
    "            rel_path = os.path.relpath(root_dir, images_src_dir)\n",
    "            dst = os.path.join(images_dst_dir, rel_path, img_file)\n",
    "            os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "            shutil.copy2(src, dst)\n",
    "            copied += 1\n",
    "            found = True\n",
    "            break\n",
    "    \n",
    "    if not found:\n",
    "        not_found.append(img_file)\n",
    "\n",
    "\n",
    "print(f\"\\nCopied {copied}/{len(df_sampled)} files\")\n",
    "if not_found:\n",
    "    print(f\"Warning: {len(not_found)} files not found\")\n",
    "    print(f\"First few missing files: {not_found[:5]}\")\n",
    "\n",
    "# save sampled auxiliary data\n",
    "output_csv = os.path.join(output_dir, \"aux_data.csv\")\n",
    "df_sampled.to_csv(output_csv, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ff1a5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 44 unique typhoons in sampled data\n",
      "Original metadata entries: 1099\n",
      "Filtered metadata entries: 44\n"
     ]
    }
   ],
   "source": [
    "# get sampled typhoon IDs\n",
    "import json\n",
    "\n",
    "sampled_csv = os.path.join(output_dir, \"aux_data.csv\")\n",
    "if not os.path.exists(sampled_csv):\n",
    "    print(f\"ERROR: {sampled_csv} not found!\")\n",
    "    print(\"Please run the main sampling script first.\")\n",
    "    exit(1)\n",
    "\n",
    "df_sampled = pd.read_csv(sampled_csv)\n",
    "sampled_typhoon_ids = sorted(df_sampled['id'].unique())\n",
    "\n",
    "print(f\"Found {len(sampled_typhoon_ids)} unique typhoons in sampled data\")\n",
    "\n",
    "# Copy METADATA folder\n",
    "metadata_src = os.path.join(root, \"metadata\")\n",
    "metadata_dst = os.path.join(output_dir, \"metadata\")\n",
    "\n",
    "if os.path.exists(metadata_src):\n",
    "    \n",
    "    # Remove existing metadata folder if it exists\n",
    "    if os.path.exists(metadata_dst):\n",
    "        print(f\"Removing existing metadata folder: {metadata_dst}\")\n",
    "        shutil.rmtree(metadata_dst)\n",
    "    \n",
    "    # Copy entire metadata folder\n",
    "    shutil.copytree(metadata_src, metadata_dst)\n",
    "\n",
    "# Filter METADATA.JSON\n",
    "metadata_json_src = os.path.join(root, \"metadata.json\")\n",
    "metadata_json_dst = os.path.join(output_dir, \"metadata.json\")\n",
    "\n",
    "if os.path.exists(metadata_json_src):\n",
    "    \n",
    "    # Load original metadata\n",
    "    with open(metadata_json_src, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"Original metadata entries: {len(metadata)}\")\n",
    "    \n",
    "    # Filter for sampled typhoons\n",
    "    # Convert sampled IDs to strings for comparison (JSON keys are strings)\n",
    "    sampled_ids_str = {str(tid) for tid in sampled_typhoon_ids}\n",
    "    \n",
    "    filtered_metadata = {\n",
    "        key: value \n",
    "        for key, value in metadata.items() \n",
    "        if key in sampled_ids_str\n",
    "    }\n",
    "    \n",
    "    print(f\"Filtered metadata entries: {len(filtered_metadata)}\")\n",
    "    \n",
    "    # Save filtered metadata\n",
    "    with open(metadata_json_dst, 'w') as f:\n",
    "        json.dump(filtered_metadata, f, indent=2)\n",
    "    \n",
    "    # Verify\n",
    "    if len(filtered_metadata) != len(sampled_typhoon_ids):\n",
    "        print(\"WARNING: Mismatch between sampled typhoons and metadata entries!\")\n",
    "        print(f\"  Sampled typhoons: {len(sampled_typhoon_ids)}\")\n",
    "        print(f\"  Metadata entries: {len(filtered_metadata)}\")\n",
    "        \n",
    "        missing = sampled_ids_str - set(filtered_metadata.keys())\n",
    "        if missing:\n",
    "            print(f\"  Missing metadata for: {missing}\")\n",
    "else:\n",
    "    print(f\"WARNING: metadata.json not found at {metadata_json_src}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64f6ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 189364 records.\n",
      "Selected 44 typhoons.\n",
      "Sampled 2080 images.\n"
     ]
    }
   ],
   "source": [
    "# Import the sampling functions\n",
    "from sample import load_data, sample_typhoons, sample_images, copy_images, save_sampled_data, copy_metadata\n",
    "\n",
    "# Set paths and parameters\n",
    "root = \"/home/ogallo/DL4CV/DigitalTyphoon/WP\"\n",
    "output_dir = \"/home/ogallo/DL4CV/WP_sampled_50\"\n",
    "total_typhoons = 50\n",
    "images_per_typhoon = 50\n",
    "\n",
    "# load data\n",
    "df = load_data(root)\n",
    "print(f\"Loaded {len(df)} records.\")\n",
    "\n",
    "# Sample typhoons\n",
    "sampled_typhoons = sample_typhoons(df, total_typhoons)\n",
    "print(f\"Selected {len(sampled_typhoons)} typhoons.\")\n",
    "\n",
    "# Sample images\n",
    "df_sampled = sample_images(df, sampled_typhoons, images_per_typhoon)\n",
    "print(f\"Sampled {len(df_sampled)} images.\")\n",
    "\n",
    "# Copy images\n",
    "copied, not_found = copy_images(df_sampled, root, output_dir)\n",
    "print(f\"Copied {copied}/{len(df_sampled)} images.\")\n",
    "if not_found:\n",
    "    print(f\"Warning: {len(not_found)} images not found.\")\n",
    "\n",
    "# Save sampled data\n",
    "save_sampled_data(df_sampled, output_dir)\n",
    "\n",
    "# Copy metadata\n",
    "sampled_typhoon_ids = sorted(df_sampled['id'].unique())\n",
    "copy_metadata(root, output_dir, sampled_typhoon_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b58e3e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchgeo.datasets import DigitalTyphoon\n",
    "dataset2 = DigitalTyphoon(\n",
    "    root=\"/home/ogallo/DL4CV/WP_sampled_50\",\n",
    "    features=[\"wind\", \"pressure\"],\n",
    "    targets=[\"wind\", \"pressure\"],\n",
    "    sequence_length=1,\n",
    "    download=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
