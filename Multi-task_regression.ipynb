{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "343d83c4",
   "metadata": {},
   "source": [
    "### Deep Learning for Computer Vision  \n",
    "### Multi-Task Regression with the Digital Typhoon Dataset\n",
    "\n",
    "This notebook demonstrates a **supervised multi-task regression** workflow for remote sensing using **TorchGeo** using the Digital Typhoon dataset, which consists of infrared (IR) satellite imagery of tropical cyclones paired with meteorological measurements.\n",
    "\n",
    "The objective is to predict multiple continuous typhoon intensity variables from satellite imagery using a deep learning model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa406dec-9786-4a3a-bb24-71a692843e0f",
   "metadata": {},
   "source": [
    "#### Dataset Overview\n",
    "The [Digital Typhoon](https://torchgeo.readthedocs.io/en/stable/api/datasets.html#digital-typhoon) is derived from hourly infrared channel observations captured by multiple generations of the Himawari meteorological satellites, spanning the period from 1978 to the present. The satellite measurements have been converted to brightness temperatures and normalized across different sensors, resulting in a consistent spatio-temporal dataset covering more than four decades.  \n",
    "\n",
    "**Dataset features:**\n",
    "- Infrared (IR) satellite imagery of 512 Ã— 512 pixels at ~5km resolution \n",
    "- Auxiliary metadata including wind speed, pressure and additional typhoon-related attributes  \n",
    "- 1,099 typhoons and 189,364 images\n",
    "\n",
    "**References**  \n",
    "Digital Typhoon Dataset: *A Large-Scale Benchmark for Tropical Cyclone Analysis*      [arXiv:2411.16421](https://arxiv.org/pdf/2411.16421) ; [arXiv:2311.02665](https://arxiv.org/pdf/2311.02665)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cfcd2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchgeo.datasets import DigitalTyphoon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6f21ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "root = \"/home/ogallo/DL4CV/DigitalTyphoon\"\n",
    "\n",
    "dataset = DigitalTyphoon(\n",
    "    root=root,\n",
    "    features=[\"wind\", \"pressure\"],\n",
    "    targets=[\"wind\", \"pressure\"],\n",
    "    sequence_length=1,\n",
    "    download=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc18a526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173418\n",
      "{'image': tensor([[[0.7248, 0.7813, 0.7813,  ..., 0.9331, 0.9363, 0.9331],\n",
      "         [0.7248, 0.7576, 0.7735,  ..., 0.9331, 0.9331, 0.9331],\n",
      "         [0.7290, 0.7536, 0.7656,  ..., 0.9299, 0.9299, 0.9331],\n",
      "         ...,\n",
      "         [0.6904, 0.6495, 0.6007,  ..., 0.8483, 0.8798, 0.8659],\n",
      "         [0.6542, 0.6400, 0.6725,  ..., 0.8483, 0.8659, 0.8447],\n",
      "         [0.7373, 0.7536, 0.7967,  ..., 0.8518, 0.8518, 0.8483]]]), 'wind': tensor(-1.1229), 'pressure': tensor(0.5422), 'label': tensor([-1.1229,  0.5422])}\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))        # number of sequences\n",
    "print(dataset[0])          # inspect the first sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e74c63c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id                   image_path  year  month  day  hour  grade    lat  \\\n",
      "0  197830  1978120100-197830-GMS1-1.h5  1978     12    1     0      6  36.00   \n",
      "1  197830  1978120103-197830-GMS1-1.h5  1978     12    1     3      6  37.46   \n",
      "2  197830  1978120106-197830-GMS1-1.h5  1978     12    1     6      6  39.00   \n",
      "3  197901  1978123112-197901-GMS1-1.h5  1978     12   31    12      2   2.00   \n",
      "4  197901  1978123116-197901-GMS1-1.h5  1978     12   31    16      2   2.30   \n",
      "\n",
      "      lng  pressure  wind  dir50  long50  short50  dir30  long30  short30  \\\n",
      "0  174.00     996.0   0.0      0       0        0      0       0        0   \n",
      "1  176.44     994.0   0.0      0       0        0      0       0        0   \n",
      "2  179.00     992.0   0.0      0       0        0      0       0        0   \n",
      "3  172.00    1004.0   0.0      0       0        0      0       0        0   \n",
      "4  171.81    1002.7   0.0      0       0        0      0       0        0   \n",
      "\n",
      "   landfall  intp  \n",
      "0         0     0  \n",
      "1         0     1  \n",
      "2         0     0  \n",
      "3         0     0  \n",
      "4         0     1  \n"
     ]
    }
   ],
   "source": [
    "aux_data = pd.read_csv(\"/home/ogallo/DL4CV/DigitalTyphoon/WP/aux_data.csv\")\n",
    "print(aux_data.head())     # inspect auxiliary data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d76f04",
   "metadata": {},
   "source": [
    "### Subset the dataset\n",
    "This is based on the typhoon grade, number of typhoons and lifecycle??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b64f6ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 189364 records from 1099 typhoons.\n",
      "Year range: 1978 - 2022\n",
      "\n",
      "Selected 110 typhoons.\n",
      "Sampled 18686 images across 110 typhoons.\n",
      "\n",
      "Copied 18686/18686 images.\n",
      "Saved aux_data.csv\n",
      "Copied and filtered metadata.\n"
     ]
    }
   ],
   "source": [
    "# Import the sampling functions\n",
    "from sample_v2 import load_data, sample_typhoons, sample_images, copy_images, save_sampled_data, copy_metadata\n",
    "\n",
    "# Set paths and parameters\n",
    "root = \"/home/ogallo/DL4CV/DigitalTyphoon/WP\"\n",
    "output_dir = \"/home/ogallo/DL4CV/WP_sampled_10pct\"\n",
    "total_typhoons = 110  # 10% of 1099 typhoons\n",
    "\n",
    "# Load data\n",
    "df = load_data(root)\n",
    "print(f\"Loaded {len(df)} records from {df['id'].nunique()} typhoons.\")\n",
    "print(f\"Year range: {df['year'].min()} - {df['year'].max()}\")\n",
    "\n",
    "# Sample typhoons (distributed across years)\n",
    "sampled_typhoons = sample_typhoons(df, total_typhoons, seed=42)\n",
    "print(f\"\\nSelected {len(sampled_typhoons)} typhoons.\")\n",
    "\n",
    "# Sample all images for selected typhoons (no cap)\n",
    "df_sampled = sample_images(df, sampled_typhoons)\n",
    "print(f\"Sampled {len(df_sampled)} images across {df_sampled['id'].nunique()} typhoons.\")\n",
    "\n",
    "# Copy images\n",
    "copied, not_found = copy_images(df_sampled, root, output_dir)\n",
    "print(f\"\\nCopied {copied}/{len(df_sampled)} images.\")\n",
    "if not_found:\n",
    "    print(f\"Warning: {len(not_found)} images not found.\")\n",
    "\n",
    "# Save sampled data\n",
    "save_sampled_data(df_sampled, output_dir)\n",
    "print(f\"Saved aux_data.csv\")\n",
    "\n",
    "# Copy metadata\n",
    "sampled_typhoon_ids = sorted(df_sampled['id'].unique())\n",
    "copy_metadata(root, output_dir, sampled_typhoon_ids)\n",
    "print(f\"Copied and filtered metadata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b58e3e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DigitalTyphoon(\n",
    "    root=\"/home/ogallo/DL4CV/WP_sampled_100\",\n",
    "    features=[\"wind\", \"pressure\"],\n",
    "    targets=[\"wind\", \"pressure\"],\n",
    "    sequence_length=1,\n",
    "    download=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01fb31-7d71-43e8-a7f5-8c564df0025d",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# visualize input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958fbee7-90f4-4f3f-88b1-8b84d1ad2be9",
   "metadata": {},
   "source": [
    "### ResNet34\n",
    "understand and explain model architecture with 2 head for multitask regression (add pic/infograph if possible)\n",
    "\n",
    "in markdown also include what we will do train, val and test then tuning???\n",
    "\n",
    "**wandb logger?? torchgeo.logger......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424d19d7-eeaf-4b93-b3fc-a9ddf8e52e80",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e66d777e-5ecd-4821-bab3-782dde004701",
   "metadata": {},
   "source": [
    "split train/val/test  \n",
    "hyperparameter tuning  \n",
    "RMSE, MSE loss etc  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
